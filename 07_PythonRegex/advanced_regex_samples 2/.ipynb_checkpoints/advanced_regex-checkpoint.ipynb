{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Regular expressions\n",
    "More ways to use regular expressions...\n",
    "\n",
    "More ways to think about parsing and reading unstructured texts..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parsing an entire text** with groups and split\n",
    "\n",
    "Open this text file for Hamlet and take a look at it! The text is very basic. \n",
    "What I do below is look for the various patterns that begin to group the parts of the play together.\n",
    "\n",
    "Plays are well-structured texts, in beautiful soup or JavaScript object notation we would understand play being organized by:\n",
    "\tplay.act.scene.dialogue_stageDirection\n",
    "    These are the levels of organization of a play.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('hamlet.txt', 'r', encoding='utf8')\n",
    "play = f.read()\n",
    "play[:500]\n",
    "#That last line just shows us the first 500 characters of the play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GROUPING PATTERNS**\n",
    "\n",
    "We can isolate information types using regular expressions. Here are a bunch of different regular expressions using groups ( ) and re.findall() that pull out every instance of a pattern. Try them out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This gets a list of characters\n",
    "# all_chars = re.findall(r\"[\\n]([A-Z ]+)[\\n]\",play)\n",
    "# all_chars\n",
    "\n",
    "#Gets a list of act names\n",
    "# act_names = re.findall(r\"[\\n](ACT [VI]+)[\\n]\",play)\n",
    "# act_names\n",
    "\n",
    "#Gets a list of act and scene names\n",
    "# act_scene=re.findall(r\"[\\n](ACT [IV]+[\\n]+SCENE [IVX]+\\.)\",play)\n",
    "# act_scene\n",
    "\n",
    "#List of scene names\n",
    "# all_scenes = re.findall(r\"(SCENE [IVX]+)\",play)\n",
    "# all_scenes\n",
    "\n",
    "#List of all acts and all scenes with acts (with a blank when ACT doesn't appear)\n",
    "# act_w_scene = re.findall(r\"(ACT [IV]+)?[\\n]+(SCENE [IVX]+)\",play)\n",
    "# act_w_scene\n",
    "\n",
    "# #List of all acts all scenes plus the scene description\n",
    "act_w_scene_des = re.findall(r\"(ACT [IV]+)?[\\n]+(SCENE [IVX]+\\.)(.+)[\\n]\",play)\n",
    "act_w_scene_des\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**regex split()**\n",
    "\n",
    "If you use split() with groups ( ) it will remember the patterns you are using to split by, but it will also isolate everything between those patterns as well! So now you are getting an organized list of all of the components of this play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVERYTHING!!! this one regular expression, using split, \n",
    "#parses the entire structure of the play.\n",
    "#It gets a list that has the act, the scene, the scene description, \n",
    "#and the entirety of that scene--each as a separate list element\n",
    "#so every fourth element contains the complete text of a scene.\n",
    "scenes = re.split(r\"(ACT [IV]+)*[\\n]+(SCENE [IVX]+[.])(.+)[\\n]\",play)\n",
    "\n",
    "#print(len(scenes))\n",
    "scenes[21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Making that list into a useful dictionary!**\n",
    "\n",
    "A good thing to understand: **lists** are great for isolating and ordering a series of data.\n",
    "\n",
    "Whereas **dictionaries** are great for grouping that data into units and making those units more meaningful by assigning keys to them.\n",
    "\n",
    "The list I get from the regular expression above is extremely useful because it has every component part of the play isolated and in a series. But when I transform it into a dictionary below, then I get a **list of every scene**: each *scene* is a **dictionary** with **keys** for the *act number*, the *scene number*, the *scene description*, and all of the *dialogue* (and stage directions) from that scene within a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The list we get from that regular expression give us a pattern, \n",
    "#(after the first element [0] which is just the title of the play.)\n",
    "\n",
    "# The pattern is: act [1], scene [2], description [3], dialogue [4]\n",
    "#                 act [5], scene [6], description [7], dialogue [8]\n",
    "#                 act [9], scene [10], description [11], dialogue [12]\n",
    "#                 ...and so forth..\n",
    "# So starting at element 1, every 4 elements match that pattern.\n",
    "# So this loop sets the range to start x at 1 and to jump every 4 ahead, \n",
    "# and it uses x in the loop to isolate each element \n",
    "# and to enter each element into a more meaningful dictionary by keys\n",
    "# (one tricky part: the act text \"ACT I\" element is empty for all subsequent scenes\n",
    "#  until we get to the next Act so I control for that with the variable current_act)\n",
    "\n",
    "hamlet_structure=[]\n",
    "current_act = \"Will Be the Act Name\"\n",
    "for x in range(1,len(scenes),4):\n",
    "    if scenes[x] is not None:\n",
    "        current_act = scenes[x]\n",
    "    scene_dict = {}\n",
    "    scene_dict['act'] = current_act\n",
    "    scene_dict['scene'] = scenes[x+1]\n",
    "    scene_dict['setting'] = scenes[x+2]\n",
    "    scene_dict['dialogue'] = scenes[x+3]\n",
    "    hamlet_structure.append(scene_dict)\n",
    "hamlet_structure[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamlet_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'The horse the boy the car the truck went to the ocean.'\n",
    "#scenes = re.split(r\"(ACT [IV]+)*[\\n]+(SCENE [IVX]+[.])(.+)[\\n]\",play)\n",
    "new_list = re.split(r\"(the)\",sentence)\n",
    "new_list= sentence.split(\"the\")\n",
    "new_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Groups for data type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#also see this link for scraping\n",
    "#https://www.house.gov/representatives\n",
    "\n",
    "house_reps = '''1st Zeldin, Lee R 1517 LHOB (202) 225-3826 Financial Services Foreign Affairs \n",
    "2nd King, Pete R 339 CHOB (202) 225-7896  Financial Services Homeland Security Intelligence \n",
    "3rd Suozzi, Thomas D 226 CHOB (202) 225-3335  Armed Services Foreign Affairs \n",
    "4th Rice, Kathleen D 1508 LHOB (202) 225-5516  Homeland Security Veterans' Affairs \n",
    "5th Meeks, Gregory W. D 2234 RHOB (202) 225-3461  Financial Services Foreign Affairs \n",
    "6th Meng, Grace D 1317 LHOB (202) 225-2601  Appropriations \n",
    "7th Velázquez, Nydia M. D 2302 RHOB (202) 225-2361  Financial Services Natural Resources Small Business \n",
    "8th Jeffries, Hakeem D 1607 LHOB (202) 225-5936  Budget Judiciary \n",
    "9th Clarke, Yvette D. D 2058 RHOB (202) 225-6231  Energy and Commerce Small Business Ethics \n",
    "10th Nadler, Jerrold D 2109 RHOB (202) 225-5635  Judiciary \n",
    "11th Donovan, Daniel R 1541 LHOB (202) 225-3371  Foreign Affairs Homeland Security \n",
    "12th Maloney, Carolyn D 2308 RHOB (202) 225-7944  Financial Services Oversight and Government Reform \n",
    "13th Espaillat, Adriano D 1630 LHOB (202) 225-4365  Education and the Workforce Foreign Affairs Small Business \n",
    "14th Crowley, Joseph D 1035 LHOB (202) 225-3965  Ways and Means \n",
    "15th Serrano, José E. D 2354 RHOB (202) 225-4361  Appropriations \n",
    "16th Engel, Eliot D 2462 RHOB (202) 225-2464  Foreign Affairs Energy and Commerce \n",
    "17th Lowey, Nita D 2365 RHOB (202) 225-6506  Appropriations Joint Select Committee on Budget and APPNs Process Reform \n",
    "18th Maloney, Sean Patrick D 1027 LHOB (202) 225-5441  Agriculture Transportation and Infrastructure \n",
    "19th Faso, John R 1616 LHOB (202) 225-5614  Agriculture Budget Transportation and Infrastructure \n",
    "20th Tonko, Paul D. D 2463 RHOB (202) 225-5076  Energy and Commerce Science, Space, and Technology \n",
    "21st Stefanik, Elise R 318 CHOB (202) 225-4611  Armed Services Education and the Workforce Intelligence \n",
    "22nd Tenney, Claudia R 512 CHOB (202) 225-3665  Financial Services \n",
    "23rd Reed, Tom R 2437 RHOB (202) 225-3161  Ways and Means \n",
    "24th Katko, John R 1620 LHOB (202) 225-3701  Homeland Security Transportation and Infrastructure \n",
    "25th Slaughter, Louise McIntosh - Vacancy D 2469 RHOB (202) 225-3615  \n",
    "26th Higgins, Brian D 2459 RHOB (202) 225-3306  Budget Ways and Means \n",
    "27th Collins, Chris R 1117 LHOB (202) 225-5265  Energy and Commerce\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_list = house_reps.splitlines()\n",
    "house_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "everything = []\n",
    "for person in house_list:\n",
    "    dists=re.findall(r\"^\\d\\d?\\w\\w\",person)[0]\n",
    "    print(dists)\n",
    "    name = re.findall(r\"\\d\\d?\\w\\w (\\D+) [DR] \\d\\d+\",person)[0]\n",
    "    print(name)\n",
    "    comm = re.findall(r\"-\\d{4} (.+)$\",person)[0]\n",
    "    print(comm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiline flag re.M allows you to search across multiple lines in a string.\n",
    "dists = re.findall(r\"^\\d\\d?\\w+\",house_reps,re.M) \n",
    "dists "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#different searching goodies in here!\n",
    "\n",
    "#[re.findall(r\"^\\d+\",line) for line in house_list]\n",
    "#[re.findall(r\"(^\\d+[nrst][dht])\",line) for line in house_list]\n",
    "#[re.findall(r\" [A-Z][\\w]+, [A-Z][\\w]+\",line) for line in house_list]\n",
    "#[re.findall(r\"^\\d+[nrst][dht] [A-Z][\\w]+,\",line) for line in house_list]\n",
    "#[re.findall(r\"[(]\\d+[)][ 0-9\\-]+\",line) for line in house_list]\n",
    "#[re.findall(r\"[(]\\d+[)] \\d{3}-\\d{4}\",line) for line in house_list]\n",
    "#[re.findall(r\"[(]\\d+[)] \\d+-*\\d+\",line) for line in house_list]\n",
    "#[re.findall(r\"\\D+$\",line) for line in house_list]\n",
    "#[re.findall(r\" [DR] \\d\",line) for line in house_list]\n",
    "#[re.findall(r\", ([A-Z]\\w+) ([A-Z]\\w+ )*([A-Z][.])*\",line) for line in house_list]\n",
    "[re.findall(r\"\\d\\d?\\w\\w (\\D+) [DR] \\d\\d+\",line) for line in house_list]\n",
    "\n",
    "#16th Engel, Eliot D 2462 RHOB (202) 225-2464  Foreign Affairs Energy and Commerce ',"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Phrase search and ? (lookahead)**\n",
    "Lookheads and lookbehinds and negative lookahead/behind are more advanced and you should just learn them when you need to use them. Basically they search for patterns without moving on to new characters. They just see if patterns happen ahead or behind or don't happen. Here is a very simple example. But when the time comes for using these you will know. This is a decent explanation online: https://www.rexegg.com/regex-lookarounds.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = re.findall(r\"\\w{2}\",\"hello I am a string\") \n",
    "phrases = re.findall(r\"(?=(\\w{2}))\",\"hello I am a string\") \n",
    "phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = '''Tomorrow, and tomorrow, and tomorrow,\n",
    "Creeps in this petty pace from day to day,\n",
    "To the last syllable of recorded time;\n",
    "And all our yesterdays have lighted fools\n",
    "The way to dusty death. Out, out, brief candle!\n",
    "Life's but a walking shadow, a poor player,\n",
    "That struts and frets his hour upon the stage,\n",
    "And then is heard no more. It is a tale\n",
    "Told by an idiot, full of sound and fury,\n",
    "Signifying nothing.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a search for three word phrases, that begin with two letter words. See the difference between the lookahead version and the regular version:\n",
    "\n",
    "With the lookahead, in the line `And then is heard no more. It is a tale` it finds all of the phrases that begin with two letter words even those that overlap:\n",
    "```\n",
    "'is heard no',\n",
    " 'no more. It',\n",
    " 'It is a',\n",
    " 'is a tale',\n",
    "```\n",
    "Without the lookahead, once it finds a pattern it moves on to the next character location, so it doesn't search for any of the two letter words that are already included in the each result:\n",
    "\n",
    "```\n",
    "'is heard no',\n",
    " 'It is a',\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# three-word phrases that begin with two-letter words\n",
    "phrases = re.findall(r\"\\b\\w{2}\\W+\\w+\\W+\\w+\",speech)\n",
    "#overlapping\n",
    "phrases = re.findall(r\"(?=(\\b\\w{2}\\W+\\w+\\W+\\w+))\",speech) \n",
    "\n",
    "\n",
    "#groups--I'm ready access but I'm using groups to isolate each word.\n",
    "#phrases = re.findall(r\"(\\b\\w{2})\\W+(\\w+)\\W+(\\w+)\",speech)\n",
    "#phrases = re.findall(r\"(?=(\\b\\w{2})\\W+(\\w+)\\W+(\\w+))\",speech)\n",
    "\n",
    "phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "**Splitting a sentence**\n",
    "This is tricky, and this isn't even the best or most robust regular expression. But it does work on an annoying group of sentences like this. Depending on the kind of text you are parsing it can be virtually impossible to accurately 100% all of the time split by sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_sentence = '''\n",
    "Ms. Smith bought cheapsite.com for 1.5 million \n",
    "dollars, i.e. he paid a lot for it. Did he \n",
    "mind? Adam Jones Jr. thinks he didn't want to. In any \n",
    "case, this isn't true... Well, with a \n",
    "probability of .9 it isn't. Right?! My name is Jonathan. Mr. Comey of \n",
    "the F.B.I. thinks not. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a regex that accurately splits \n",
    "#the paragraph above into sentences\n",
    "\n",
    "#this works ok!!\n",
    "#First group looks for:\n",
    "#     any three characters that are NOT capitalized letters followed by . or ? or !\n",
    "#           (this will not work for something that's written in all caps)\n",
    "#           (this will not even work for a sentence ending: \"said Jon.\")\n",
    "#Second group looks ahead from that first group for:\n",
    "#     one or more spaces and a Cap to start the next sentence.\n",
    "#     That second part, because it is a lookahead, does not get captured in the groups.\n",
    "#split() splits by that first pattern as long as the lookahead is true. \n",
    "\n",
    "#So we get a list 'sents' that has the pattern and the beginning of the next sentence.\n",
    "\n",
    "sents = re.split(r\"([^A-Z]{3}[.?!\\\"\\'])(?=\\s+[A-Z])\",to_sentence)\n",
    "sents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we have to join every other part of the 'sents' list together \n",
    "#to re-combine the full sentence.\n",
    "\n",
    "join_sents = [sents[x] + sents[x+1] for x in range(0,len(sents)-2,2)]\n",
    "join_sents.append(sents[-1])\n",
    "join_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More problems with text: The Waste Land**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('wasteland.txt', 'r', encoding='utf8')\n",
    "wasteland = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?! Looked ahead for something not containing\n",
    "# Homework example of 2 occurrences \"ow\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sometimes phrases are more useful than word searches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = re.findall(r\"\\b\\w{2}\\W+\\w+\\W+\\w+\",wasteland)\n",
    "phrases = re.findall(r\"\\bof\\W+the\\W+\\w+\",wasteland,re.IGNORECASE)\n",
    "phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Writing a Most Frequent Words script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waste_words = wasteland.lower()\n",
    "\n",
    "#get a list of words\n",
    "#waste_words1 = re.split(r\"\\W+\",waste_words)\n",
    "waste_words2 = re.findall(r\"\\b\\w+\\b'?[ts]?\",waste_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waste_words2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort words alphabetically\n",
    "sortwords = waste_words2.copy()\n",
    "sortwords.sort()\n",
    "sortwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through the alphabetically arranged list\n",
    "#count each instance of the word and make a dictionary\n",
    "all_words = []\n",
    "counter = 1\n",
    "this_word = \"a\"\n",
    "for word in sortwords:\n",
    "    if word != this_word:\n",
    "        all_words.append({'word':this_word,'count':counter})\n",
    "        counter = 1\n",
    "        this_word = word\n",
    "    else:\n",
    "        counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort a dictionary by a key's value\n",
    "order_words = sorted(all_words, key=lambda d: d['count'], reverse=True)\n",
    "order_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#more efficient dictionary way, as a function:\n",
    "\n",
    "def word_freq(my_text):\n",
    "    word_dict = {}\n",
    "    words = re.findall(r\"\\b\\w+\\b\", my_text.lower())\n",
    "    for word in words:\n",
    "        if word in word_dict:\n",
    "            word_dict[word] += 1\n",
    "        else:\n",
    "            word_dict[word] = 1\n",
    "    return dict(sorted(word_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = word_freq(waste_words)\n",
    "word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Or use the Built-in version!!!!!\n",
    "from collections import Counter\n",
    "wordcount = Counter(waste_words2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcount.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
